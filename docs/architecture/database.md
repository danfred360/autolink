Technology: Autolink uses a PostgreSQL relational database running in a Docker container. PostgreSQL was chosen for its reliability, rich feature set (JSON support, robust indexing, constraints), and alignment with our structured data needs. Running it in Docker makes local development and initial deployment simple, and it provides environment parity (we can replicate the same DB image in production).

Schema Design: The database schema is designed to capture users and their automobile data with appropriate normalization. Key tables and relationships include:
	•	Users – stores user account info (UserID, name, email, hashed password, etc.). Each user can own multiple cars.
	•	Cars – stores car details (CarID, owner UserID as a foreign key, make, model, year, VIN, etc.). A User-to-Car relationship is one-to-many.
	•	MaintenanceRecords – logs maintenance entries for cars (RecordID, CarID FK, date, odometer reading, type of service, description, cost, etc.). A Car can have many maintenance records.
	•	FuelUps – logs fuel purchases (FuelID, CarID FK, date, odometer, fuel volume, price, etc.) for tracking fuel economy. A Car can have many fuel-up records.
	•	Parts – an optional table listing car parts or components (PartID, name, part number, etc.), used for tracking parts replaced during maintenance. This can be linked to maintenance records through a join table.
	•	MaintenanceParts – a join table mapping maintenance records to parts used (RecordID FK, PartID FK, quantity, cost). This captures which parts were replaced in a given maintenance event, since a single maintenance record (e.g. a repair) could involve multiple parts.

This schema normalizes data to avoid duplication (e.g. parts data is stored once). We use foreign key constraints (with cascade options as appropriate) to maintain referential integrity – for example, if a car is deleted, optionally its maintenance and fuel records could be cascade-deleted. Common queries (like fetching all maintenance for a car or all cars for a user) are efficient due to indexed foreign keys (explained below). The design is flexible to accommodate future expansion, such as adding a Service Center table if we track where maintenance was performed, or a Reminders table for upcoming service reminders, without altering existing tables.

Partitioning & Indexing Strategies: To ensure good performance, we rely primarily on proper indexing and will consider table partitioning only if data volume grows very large. Every table has primary key indexes and foreign key indexes (e.g., indexes on CarID in MaintenanceRecords and FuelUps, and on UserID in Cars) to speed up joins and lookups. We add additional indexes on frequently queried fields: for instance, an index on MaintenanceRecords(date) to quickly retrieve records within a date range, and perhaps a composite index on FuelUps(CarID, date) if we often query fuel-ups per car in chronological order. These indexes are chosen based on query patterns to optimize read performance.

For partitioning, our main candidate tables are the time-series logs like MaintenanceRecords or FuelUps, which could accumulate a large number of rows over time. We could use range partitioning by year or by quarter on the date field so that older data is in separate partitions. Partitioning can improve query performance when the query can be pruned to a few partitions (e.g. querying recent data) and keeps indexes smaller (only relevant data in memory) ￼. It also makes data management easier – for example, dropping an old partition to purge data is much faster than a massive DELETE ￼. However, these benefits only pay off when tables grow extremely large (on the order of millions of rows, exceeding available memory) ￼. We will likely not partition initially, since premature partitioning can complicate the schema and even degrade query performance for moderate-sized data sets ￼. Instead, we’ll first rely on well-chosen indexes and periodic archiving of old records if needed. As Autolink usage grows, we will monitor query performance – if we approach the threshold where a table becomes very large (rule of thumb: larger than RAM) ￼, we can introduce partitioning. An alternative considered was no partitioning with active archiving (moving old data to an archive table), which we may do for data retention purposes. In summary, our indexing strategy covers all primary and foreign keys and common filters, and partitioning is a future scalability measure once data size justifies it.

Proxying Strategy (Express as DB Proxy): The Express backend service acts as the only gateway to the database – the client app never talks to Postgres directly. All database interactions (queries and updates) are done through the API server, which serves as a proxy layer. This design centralizes security and data access logic: the API can authenticate requests, enforce authorization rules, validate inputs, and only then construct SQL queries. We use parameterized queries or an ORM to interact with Postgres, which protects against SQL injection by never directly concatenating user input into queries ￼. For example, using a query builder or parameter binding ensures that special characters in inputs (like quotes) are properly escaped by the driver, safeguarding the database. The Express proxy also enables us to implement business logic (e.g. calculating derived fields, enforcing business rules) in one place before committing to the database. In the future, if we introduce caching or read replicas, the proxy can decide when to serve data from cache versus querying the primary database. By keeping the database behind a controlled API, we maintain a clean separation of concerns and add an extra layer of security and logging for all DB operations.